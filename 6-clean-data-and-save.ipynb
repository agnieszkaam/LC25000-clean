{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44e19b7",
   "metadata": {},
   "source": [
    "# Data Cleaning Pipeline\n",
    "\n",
    "This notebook cleans the LC25000 dataset using the final clustering results from manual annotation. The cleaning process removes duplicates and contaminated samples based on the clustering analysis performed in the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05ee49b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import project constants\n",
    "from source.constants import (\n",
    "    PROJECT_PATH, DATA_DIR, ALL_CANCER_TYPES, \n",
    "    ANNOTATIONS_SAVE_DIR, NUM_CLASS_PROTOTYPES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3c9f782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path: /Users/agnieszka/Poznan/project-2-imaging/lc-25k-cleaning/LC25000-clean\n",
      "Data directory: /Users/agnieszka/Poznan/project-2-imaging/lc-25k-cleaning/LC25000-clean/LC25000\n",
      "Annotations directory: /Users/agnieszka/Poznan/project-2-imaging/lc-25k-cleaning/LC25000-clean/annotations\n",
      "Cleaning Mode: clean\n",
      "Output directory: /Users/agnieszka/Poznan/project-2-imaging/lc-25k-cleaning/LC25000-clean/LC25000_Clean\n",
      "Cancer types to process: ('colon_aca', 'colon_n', 'lung_aca', 'lung_n', 'lung_scc')\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "EXTRACTOR_NAME = \"UNI\"  # Feature extractor used for clustering\n",
    "IMG_NORM = \"resize_only\"  # Image normalization method used\n",
    "\n",
    "# Set the mode: 'clean' to de-duplicate, 'organize' to keep all images and annotate with cluster ID\n",
    "CLEANING_MODE = 'clean'  # Options: 'clean', 'organize'\n",
    "\n",
    "# Define output directory based on the mode\n",
    "if CLEANING_MODE == 'clean':\n",
    "    OUTPUT_DIR = os.path.join(PROJECT_PATH, \"LC25000_Clean\")\n",
    "elif CLEANING_MODE == 'organize':\n",
    "    OUTPUT_DIR = os.path.join(PROJECT_PATH, \"LC25000_Organized\")\n",
    "else:\n",
    "    raise ValueError(\"CLEANING_MODE must be either 'clean' or 'organize'\")\n",
    "\n",
    "print(f\"Project path: {PROJECT_PATH}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Annotations directory: {ANNOTATIONS_SAVE_DIR}\")\n",
    "print(f\"Cleaning Mode: {CLEANING_MODE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Cancer types to process: {ALL_CANCER_TYPES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb660246",
   "metadata": {},
   "source": [
    "## Step 1: Load Clustering Results\n",
    "\n",
    "Load the final clustering results for each cancer type. These clusters were created through the semi-automatic annotation process in notebook 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd02df73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colon_aca: 249 clusters, 5000 images\n",
      "colon_n: 257 clusters, 5000 images\n",
      "lung_aca: 243 clusters, 5000 images\n",
      "lung_n: 249 clusters, 5000 images\n",
      "lung_scc: 248 clusters, 5000 images\n",
      "\n",
      "Total: 1246 clusters, 25000 images across all cancer types\n"
     ]
    }
   ],
   "source": [
    "def load_final_clusters_csv(cancer_type, extractor_name, img_norm):\n",
    "    \"\"\"Load final clustering results from CSV file for a specific cancer type.\"\"\"\n",
    "    annotations_dir = os.path.join(ANNOTATIONS_SAVE_DIR, cancer_type, extractor_name, img_norm)\n",
    "    final_clusters_csv_path = os.path.join(annotations_dir, \"final_clusters.csv\")\n",
    "    \n",
    "    if not os.path.exists(final_clusters_csv_path):\n",
    "        print(f\"Warning: No clustering results found at {final_clusters_csv_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(final_clusters_csv_path)\n",
    "    \n",
    "    # Group by cluster_label to create clusters dictionary\n",
    "    clusters = {}\n",
    "    for cluster_id, group in df.groupby('cluster_label'):\n",
    "        # Convert relative paths to absolute paths\n",
    "        img_paths = []\n",
    "        for img_path in group['img_path']:\n",
    "            # Remove the './' prefix and make absolute path\n",
    "            clean_path = img_path.replace('./', '')\n",
    "            abs_path = os.path.join(PROJECT_PATH, clean_path)\n",
    "            img_paths.append(abs_path)\n",
    "        clusters[cluster_id] = img_paths\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "# Load clustering results for all cancer types from CSV files\n",
    "all_clusters = {}\n",
    "total_clusters = 0\n",
    "total_images = 0\n",
    "\n",
    "for cancer_type in ALL_CANCER_TYPES:\n",
    "    clusters = load_final_clusters_csv(cancer_type, EXTRACTOR_NAME, IMG_NORM)\n",
    "    if clusters is not None:\n",
    "        all_clusters[cancer_type] = clusters\n",
    "        total_clusters += len(clusters)\n",
    "        total_images += sum(len(cluster_images) for cluster_images in clusters.values())\n",
    "        print(f\"{cancer_type}: {len(clusters)} clusters, {sum(len(cluster_images) for cluster_images in clusters.values())} images\")\n",
    "    else:\n",
    "        print(f\"No clusters found for {cancer_type}\")\n",
    "\n",
    "print(f\"\\nTotal: {total_clusters} clusters, {total_images} images across all cancer types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c0f7c",
   "metadata": {},
   "source": [
    "## Step 2: Analyze Cluster Sizes\n",
    "\n",
    "Analyze the distribution of cluster sizes to understand the data better before cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56ad2dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COLON_ACA:\n",
      "  Total clusters: 249\n",
      "  Total images: 5000\n",
      "  Cluster size - Min: 6, Max: 34\n",
      "  Cluster size - Mean: 20.08, Median: 20.00\n",
      "  Size distribution: {6: 1, 9: 1, 10: 2, 11: 3, 12: 2, 13: 7, 14: 5, 15: 12, 16: 24, 17: 22}...\n",
      "\n",
      "COLON_N:\n",
      "  Total clusters: 257\n",
      "  Total images: 5000\n",
      "  Cluster size - Min: 1, Max: 53\n",
      "  Cluster size - Mean: 19.46, Median: 20.00\n",
      "  Size distribution: {1: 6, 2: 2, 4: 1, 5: 1, 6: 1, 7: 2, 8: 1, 9: 2, 10: 3, 11: 2}...\n",
      "\n",
      "LUNG_ACA:\n",
      "  Total clusters: 243\n",
      "  Total images: 5000\n",
      "  Cluster size - Min: 7, Max: 45\n",
      "  Cluster size - Mean: 20.58, Median: 20.00\n",
      "  Size distribution: {7: 1, 9: 1, 11: 1, 12: 7, 13: 6, 14: 9, 15: 16, 16: 17, 17: 10, 18: 18}...\n",
      "\n",
      "LUNG_N:\n",
      "  Total clusters: 249\n",
      "  Total images: 5000\n",
      "  Cluster size - Min: 3, Max: 46\n",
      "  Cluster size - Mean: 20.08, Median: 20.00\n",
      "  Size distribution: {3: 1, 7: 1, 9: 2, 10: 1, 11: 2, 12: 4, 13: 4, 14: 16, 15: 11, 16: 17}...\n",
      "\n",
      "LUNG_SCC:\n",
      "  Total clusters: 248\n",
      "  Total images: 5000\n",
      "  Cluster size - Min: 8, Max: 45\n",
      "  Cluster size - Mean: 20.16, Median: 20.00\n",
      "  Size distribution: {8: 1, 10: 1, 11: 2, 12: 5, 13: 9, 14: 14, 15: 11, 16: 15, 17: 19, 18: 27}...\n"
     ]
    }
   ],
   "source": [
    "def analyze_cluster_sizes(all_clusters):\n",
    "    \"\"\"Analyze the distribution of cluster sizes across all cancer types.\"\"\"\n",
    "    \n",
    "    cluster_stats = {}\n",
    "    \n",
    "    for cancer_type, clusters in all_clusters.items():\n",
    "        cluster_sizes = [len(cluster_images) for cluster_images in clusters.values()]\n",
    "        \n",
    "        stats = {\n",
    "            'total_clusters': len(clusters),\n",
    "            'total_images': sum(cluster_sizes),\n",
    "            'min_size': min(cluster_sizes) if cluster_sizes else 0,\n",
    "            'max_size': max(cluster_sizes) if cluster_sizes else 0,\n",
    "            'mean_size': np.mean(cluster_sizes) if cluster_sizes else 0,\n",
    "            'median_size': np.median(cluster_sizes) if cluster_sizes else 0,\n",
    "            'size_distribution': Counter(cluster_sizes)\n",
    "        }\n",
    "        \n",
    "        cluster_stats[cancer_type] = stats\n",
    "        \n",
    "        print(f\"\\n{cancer_type.upper()}:\")\n",
    "        print(f\"  Total clusters: {stats['total_clusters']}\")\n",
    "        print(f\"  Total images: {stats['total_images']}\")\n",
    "        print(f\"  Cluster size - Min: {stats['min_size']}, Max: {stats['max_size']}\")\n",
    "        print(f\"  Cluster size - Mean: {stats['mean_size']:.2f}, Median: {stats['median_size']:.2f}\")\n",
    "        \n",
    "        # Show size distribution\n",
    "        size_counts = sorted(stats['size_distribution'].items())\n",
    "        print(f\"  Size distribution: {dict(size_counts[:10])}...\")  # Show first 10\n",
    "    \n",
    "    return cluster_stats\n",
    "\n",
    "# Analyze cluster sizes\n",
    "cluster_stats = analyze_cluster_sizes(all_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c46f5",
   "metadata": {},
   "source": [
    "## Step 3: Define Processing Strategy\n",
    "\n",
    "Define the strategy for processing the dataset based on the `CLEANING_MODE` flag.\n",
    "- **clean**: Keep one representative per cluster to de-duplicate the dataset.\n",
    "- **organize**: Keep all images and annotate filenames with their cluster ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3dc13d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colon_aca: Selected 249 representative images from 249 clusters\n",
      "colon_n: Selected 257 representative images from 257 clusters\n",
      "lung_aca: Selected 243 representative images from 243 clusters\n",
      "lung_n: Selected 249 representative images from 249 clusters\n",
      "lung_scc: Selected 248 representative images from 248 clusters\n",
      "\n",
      "Total representative images selected: 1246\n"
     ]
    }
   ],
   "source": [
    "# This step is only necessary for 'clean' mode\n",
    "if CLEANING_MODE == 'clean':\n",
    "    def select_representative_images(all_clusters, selection_strategy=\"first\"):\n",
    "        \"\"\"\n",
    "        Select representative images from each cluster based on the final_clusters.csv data.\n",
    "        \n",
    "        Args:\n",
    "            all_clusters: Dictionary of clusters for each cancer type (from CSV)\n",
    "            selection_strategy: Strategy for selecting representative (\"first\", \"random\", \"median\")\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with selected representative images for each cancer type\n",
    "        \"\"\"\n",
    "        \n",
    "        representatives = {}\n",
    "        \n",
    "        for cancer_type, clusters in all_clusters.items():\n",
    "            selected_images = []\n",
    "            \n",
    "            for cluster_id, cluster_images in clusters.items():\n",
    "                if len(cluster_images) > 0:\n",
    "                    if selection_strategy == \"first\":\n",
    "                        # Select the first image in the cluster\n",
    "                        representative = cluster_images[0]\n",
    "                    elif selection_strategy == \"random\":\n",
    "                        # Select a random image from the cluster\n",
    "                        representative = np.random.choice(cluster_images)\n",
    "                    elif selection_strategy == \"median\":\n",
    "                        # Select the median image (middle index)\n",
    "                        median_idx = len(cluster_images) // 2\n",
    "                        representative = cluster_images[median_idx]\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown selection strategy: {selection_strategy}\")\n",
    "                    \n",
    "                    selected_images.append((representative, cluster_id))  # Include cluster_id for naming\n",
    "            \n",
    "            representatives[cancer_type] = selected_images\n",
    "            print(f\"{cancer_type}: Selected {len(selected_images)} representative images from {len(clusters)} clusters\")\n",
    "        \n",
    "        return representatives\n",
    "\n",
    "    # Select representative images (using first strategy)\n",
    "    representative_images = select_representative_images(all_clusters, selection_strategy=\"first\")\n",
    "\n",
    "    # Print summary\n",
    "    total_representatives = sum(len(images) for images in representative_images.values())\n",
    "    print(f\"\\nTotal representative images selected: {total_representatives}\")\n",
    "else:\n",
    "    print(\"Skipping representative selection in 'organize' mode.\")\n",
    "    representative_images = None  # Not needed for organize mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0e220d",
   "metadata": {},
   "source": [
    "## Step 4: Create Processed Dataset\n",
    "\n",
    "Copy the images to create the new dataset based on the selected mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5d1d02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset in 'clean' mode...\n",
      "\n",
      "Processing colon_aca...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying colon_aca: 100%|██████████| 249/249 [00:00<00:00, 2236.47it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colon_aca: Copied 249/249 images\n",
      "\n",
      "Processing colon_n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying colon_n: 100%|██████████| 257/257 [00:00<00:00, 2437.94it/s]\n",
      "Copying colon_n: 100%|██████████| 257/257 [00:00<00:00, 2437.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colon_n: Copied 257/257 images\n",
      "\n",
      "Processing lung_aca...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying lung_aca: 100%|██████████| 243/243 [00:00<00:00, 2700.13it/s]\n",
      "Copying lung_aca: 100%|██████████| 243/243 [00:00<00:00, 2700.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lung_aca: Copied 243/243 images\n",
      "\n",
      "Processing lung_n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying lung_n: 100%|██████████| 249/249 [00:00<00:00, 2361.67it/s]\n",
      "Copying lung_n: 100%|██████████| 249/249 [00:00<00:00, 2361.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lung_n: Copied 249/249 images\n",
      "\n",
      "Processing lung_scc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying lung_scc: 100%|██████████| 248/248 [00:00<00:00, 2581.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lung_scc: Copied 248/248 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(mode, all_clusters, representative_images, output_dir):\n",
    "    \"\"\"\n",
    "    Create a new dataset by copying images based on the selected mode.\n",
    "    \n",
    "    Args:\n",
    "        mode: 'clean' or 'organize'\n",
    "        all_clusters: Dictionary of all clusters\n",
    "        representative_images: Dictionary of representative images (for 'clean' mode)\n",
    "        output_dir: Output directory for the new dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clear the output directory if it exists\n",
    "    if os.path.exists(output_dir):\n",
    "        print(f\"Output directory {output_dir} already exists. Clearing it first.\")\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    copy_stats = {}\n",
    "    \n",
    "    for cancer_type, clusters in all_clusters.items():\n",
    "        cancer_type_dir = os.path.join(output_dir, cancer_type)\n",
    "        os.makedirs(cancer_type_dir, exist_ok=True)\n",
    "        \n",
    "        copied_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        print(f\"\\nProcessing {cancer_type}...\")\n",
    "        \n",
    "        if mode == 'clean':\n",
    "            # In 'clean' mode, only copy representatives\n",
    "            image_data = representative_images.get(cancer_type, [])\n",
    "            for image_path, cluster_id in tqdm(image_data, desc=f\"Copying {cancer_type}\"):\n",
    "                try:\n",
    "                    filename = os.path.basename(image_path)\n",
    "                    name, ext = os.path.splitext(filename)\n",
    "                    new_filename = f\"cluster_{cluster_id:03d}_{name}{ext}\"\n",
    "                    dest_path = os.path.join(cancer_type_dir, new_filename)\n",
    "                    \n",
    "                    if os.path.exists(image_path):\n",
    "                        shutil.copy2(image_path, dest_path)\n",
    "                        copied_count += 1\n",
    "                    else:\n",
    "                        print(f\"Warning: Source file not found: {image_path}\")\n",
    "                        failed_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error copying {image_path}: {e}\")\n",
    "                    failed_count += 1\n",
    "            total_to_copy = len(image_data)\n",
    "\n",
    "        elif mode == 'organize':\n",
    "            # In 'organize' mode, copy all images\n",
    "            total_to_copy = sum(len(imgs) for imgs in clusters.values())\n",
    "            for cluster_id, image_paths in tqdm(clusters.items(), desc=f\"Copying {cancer_type}\"):\n",
    "                for image_path in image_paths:\n",
    "                    try:\n",
    "                        filename = os.path.basename(image_path)\n",
    "                        name, ext = os.path.splitext(filename)\n",
    "                        new_filename = f\"cluster_{cluster_id:03d}_{name}{ext}\"\n",
    "                        dest_path = os.path.join(cancer_type_dir, new_filename)\n",
    "                        \n",
    "                        if os.path.exists(image_path):\n",
    "                            shutil.copy2(image_path, dest_path)\n",
    "                            copied_count += 1\n",
    "                        else:\n",
    "                            print(f\"Warning: Source file not found: {image_path}\")\n",
    "                            failed_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error copying {image_path}: {e}\")\n",
    "                        failed_count += 1\n",
    "        \n",
    "        copy_stats[cancer_type] = {\n",
    "            'copied': copied_count,\n",
    "            'failed': failed_count,\n",
    "            'total': total_to_copy\n",
    "        }\n",
    "        \n",
    "        print(f\"{cancer_type}: Copied {copied_count}/{total_to_copy} images\")\n",
    "    \n",
    "    return copy_stats\n",
    "\n",
    "# Create the new dataset based on the CLEANING_MODE\n",
    "print(f\"Creating dataset in '{CLEANING_MODE}' mode...\")\n",
    "copy_statistics = create_dataset(CLEANING_MODE, all_clusters, representative_images, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad6b5b",
   "metadata": {},
   "source": [
    "## Step 5: Generate Cleaning Report\n",
    "\n",
    "Create a comprehensive report of the cleaning process including statistics and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaede92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "CLEANING REPORT SUMMARY\n",
      "============================================================\n",
      "Original dataset: 25,000 images\n",
      "Cleaned dataset: 1,246 images\n",
      "Images removed: 23,754 (95.02%)\n",
      "Compression ratio: 0.050\n",
      "\\nPer-class statistics:\n",
      "  colon_aca: 5,000 → 249 (0.050)\n",
      "  colon_n: 5,000 → 257 (0.051)\n",
      "  lung_aca: 5,000 → 243 (0.049)\n",
      "  lung_n: 5,000 → 249 (0.050)\n",
      "  lung_scc: 5,000 → 248 (0.050)\n",
      "\\nDetailed report saved to: /Users/agnieszka/Poznan/project-2-imaging/lc-25k-cleaning/LC25000-clean/LC25000_Clean/cleaning_report.json\n"
     ]
    }
   ],
   "source": [
    "def generate_cleaning_report(cluster_stats, copy_statistics, output_dir):\n",
    "    \"\"\"Generate a comprehensive cleaning report.\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'cleaning_metadata': {\n",
    "            'extractor_used': EXTRACTOR_NAME,\n",
    "            'normalization_used': IMG_NORM,\n",
    "            'selection_strategy': 'first',  # cluster centroid\n",
    "            'date_cleaned': pd.Timestamp.now().isoformat(),\n",
    "            'source_directory': DATA_DIR,\n",
    "            'output_directory': output_dir\n",
    "        },\n",
    "        'original_dataset_stats': {},\n",
    "        'clustering_stats': cluster_stats,\n",
    "        'cleaned_dataset_stats': copy_statistics,\n",
    "        'cleaning_summary': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    total_original = sum(stats['total_images'] for stats in cluster_stats.values())\n",
    "    total_cleaned = sum(stats['copied'] for stats in copy_statistics.values())\n",
    "    reduction_ratio = 1 - (total_cleaned / total_original) if total_original > 0 else 0\n",
    "    \n",
    "    report['cleaning_summary'] = {\n",
    "        'total_original_images': total_original,\n",
    "        'total_cleaned_images': total_cleaned,\n",
    "        'images_removed': total_original - total_cleaned,\n",
    "        'reduction_ratio': reduction_ratio,\n",
    "        'compression_ratio': total_cleaned / total_original if total_original > 0 else 0\n",
    "    }\n",
    "    \n",
    "    # Save report as JSON\n",
    "    report_path = os.path.join(output_dir, 'cleaning_report.json')\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"CLEANING REPORT SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Original dataset: {total_original:,} images\")\n",
    "    print(f\"Cleaned dataset: {total_cleaned:,} images\")\n",
    "    print(f\"Images removed: {total_original - total_cleaned:,} ({reduction_ratio:.2%})\")\n",
    "    print(f\"Compression ratio: {total_cleaned / total_original:.3f}\" if total_original > 0 else \"N/A\")\n",
    "    \n",
    "    print(\"\\\\nPer-class statistics:\")\n",
    "    for cancer_type in ALL_CANCER_TYPES:\n",
    "        if cancer_type in cluster_stats and cancer_type in copy_statistics:\n",
    "            original = cluster_stats[cancer_type]['total_images']\n",
    "            cleaned = copy_statistics[cancer_type]['copied']\n",
    "            print(f\"  {cancer_type}: {original:,} → {cleaned:,} ({cleaned/original:.3f})\")\n",
    "    \n",
    "    print(f\"\\\\nDetailed report saved to: {report_path}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate cleaning report\n",
    "cleaning_report = generate_cleaning_report(cluster_stats, copy_statistics, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17e1a45",
   "metadata": {},
   "source": [
    "## Step 6: Create Image Mapping\n",
    "\n",
    "Create a mapping file that tracks which original images were kept and which clusters they came from. This step is adjusted based on the cleaning mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed18a23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image mapping saved to:\n",
      "  CSV: /Users/agnieszka/Poznan/project-2-imaging/lc-25k-cleaning/LC25000-clean/LC25000_Clean/image_mapping.csv\n",
      "  JSON: /Users/agnieszka/Poznan/project-2-imaging/lc-25k-cleaning/LC25000-clean/LC25000_Clean/image_mapping.json\n"
     ]
    }
   ],
   "source": [
    "def create_image_mapping(mode, all_clusters, representative_images, output_dir):\n",
    "    \"\"\"\n",
    "    Create a detailed mapping of which images were kept and which clusters they came from.\n",
    "    \"\"\"\n",
    "    \n",
    "    mapping_data = []\n",
    "    \n",
    "    for cancer_type, clusters in all_clusters.items():\n",
    "        if mode == 'clean':\n",
    "            # In 'clean' mode, map only the representatives\n",
    "            image_data = representative_images.get(cancer_type, [])\n",
    "            for image_path, cluster_id in image_data:\n",
    "                cluster_images = clusters[cluster_id]\n",
    "                mapping_entry = {\n",
    "                    'cancer_type': cancer_type,\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'representative_image': image_path,\n",
    "                    'cluster_size': len(cluster_images),\n",
    "                    'duplicate_images': [img for img in cluster_images if img != image_path],\n",
    "                    'kept_in_cleaned_dataset': True\n",
    "                }\n",
    "                mapping_data.append(mapping_entry)\n",
    "        \n",
    "        elif mode == 'organize':\n",
    "            # In 'organize' mode, every image is a \"representative\" of its cluster\n",
    "            for cluster_id, image_paths in clusters.items():\n",
    "                for image_path in image_paths:\n",
    "                    mapping_entry = {\n",
    "                        'cancer_type': cancer_type,\n",
    "                        'cluster_id': cluster_id,\n",
    "                        'image_path': image_path,\n",
    "                        'cluster_size': len(image_paths),\n",
    "                        'kept_in_organized_dataset': True\n",
    "                    }\n",
    "                    mapping_data.append(mapping_entry)\n",
    "\n",
    "    # Create DataFrame for easy analysis\n",
    "    mapping_df = pd.DataFrame(mapping_data)\n",
    "    \n",
    "    # Save as CSV\n",
    "    mapping_csv_path = os.path.join(output_dir, 'image_mapping.csv')\n",
    "    mapping_df.to_csv(mapping_csv_path, index=False)\n",
    "    \n",
    "    # Save detailed mapping as JSON\n",
    "    mapping_json_path = os.path.join(output_dir, 'image_mapping.json')\n",
    "    with open(mapping_json_path, 'w') as f:\n",
    "        json.dump(mapping_data, f, indent=4)\n",
    "    \n",
    "    print(f\"Image mapping saved to:\")\n",
    "    print(f\"  CSV: {mapping_csv_path}\")\n",
    "    print(f\"  JSON: {mapping_json_path}\")\n",
    "    \n",
    "    return mapping_df\n",
    "\n",
    "# Create image mapping\n",
    "mapping_df = create_image_mapping(CLEANING_MODE, all_clusters, representative_images, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61776d38",
   "metadata": {},
   "source": [
    "## Step 7: Validation and Quality Checks\n",
    "\n",
    "Perform validation checks on the cleaned dataset to ensure everything was processed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b33d7b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating cleaned dataset...\n",
      "  colon_aca: 249 files\n",
      "  colon_n: 257 files\n",
      "  lung_aca: 243 files\n",
      "  lung_n: 249 files\n",
      "  lung_scc: 248 files\n",
      "\n",
      "Validation PASSED - Dataset cleaned successfully!\n",
      "\n",
      "Cleaned dataset location: /Users/agnieszka/Poznan/project-2-imaging/lc-25k-cleaning/LC25000-clean/LC25000_Clean\n",
      "Total images in cleaned dataset: 1,246\n",
      "Total reduction: 23,754 images (95.0%)\n"
     ]
    }
   ],
   "source": [
    "def validate_cleaned_dataset(output_dir, expected_stats):\n",
    "    \"\"\"\n",
    "    Validate the cleaned dataset by checking file counts and integrity.\n",
    "    \"\"\"\n",
    "    \n",
    "    validation_results = {\n",
    "        'validation_passed': True,\n",
    "        'issues': [],\n",
    "        'file_counts': {},\n",
    "        'missing_files': [],\n",
    "        'unexpected_files': []\n",
    "    }\n",
    "    \n",
    "    print(\"Validating cleaned dataset...\")\n",
    "    \n",
    "    # Check if output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        validation_results['validation_passed'] = False\n",
    "        validation_results['issues'].append(f\"Output directory does not exist: {output_dir}\")\n",
    "        return validation_results\n",
    "    \n",
    "    # Check each cancer type directory\n",
    "    for cancer_type in ALL_CANCER_TYPES:\n",
    "        cancer_dir = os.path.join(output_dir, cancer_type)\n",
    "        \n",
    "        if not os.path.exists(cancer_dir):\n",
    "            # This is not a failure if no images were expected for this class\n",
    "            if cancer_type not in expected_stats or expected_stats[cancer_type]['copied'] == 0:\n",
    "                continue\n",
    "            validation_results['validation_passed'] = False\n",
    "            validation_results['issues'].append(f\"Cancer type directory missing: {cancer_dir}\")\n",
    "            continue\n",
    "        \n",
    "        # Count files in the directory\n",
    "        files = [f for f in os.listdir(cancer_dir) if os.path.isfile(os.path.join(cancer_dir, f))]\n",
    "        file_count = len(files)\n",
    "        validation_results['file_counts'][cancer_type] = file_count\n",
    "        \n",
    "        # Check against expected count\n",
    "        if cancer_type in expected_stats:\n",
    "            expected_count = expected_stats[cancer_type]['copied']\n",
    "            if file_count != expected_count:\n",
    "                validation_results['validation_passed'] = False\n",
    "                validation_results['issues'].append(\n",
    "                    f\"{cancer_type}: Expected {expected_count} files, found {file_count}\"\n",
    "                )\n",
    "        \n",
    "        print(f\"  {cancer_type}: {file_count} files\")\n",
    "    \n",
    "    # Check for required metadata files\n",
    "    required_files = ['cleaning_report.json', 'image_mapping.csv', 'image_mapping.json']\n",
    "    for req_file in required_files:\n",
    "        file_path = os.path.join(output_dir, req_file)\n",
    "        if not os.path.exists(file_path):\n",
    "            validation_results['validation_passed'] = False\n",
    "            validation_results['issues'].append(f\"Required file missing: {req_file}\")\n",
    "    \n",
    "    # Summary\n",
    "    if validation_results['validation_passed']:\n",
    "        print(\"\\nValidation PASSED - Dataset cleaned successfully!\")\n",
    "    else:\n",
    "        print(\"\\nValidation FAILED - Issues found:\")\n",
    "        for issue in validation_results['issues']:\n",
    "            print(f\"   - {issue}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Validate the cleaned dataset\n",
    "validation_results = validate_cleaned_dataset(OUTPUT_DIR, copy_statistics)\n",
    "\n",
    "# Print final summary\n",
    "print(f\"\\nCleaned dataset location: {OUTPUT_DIR}\")\n",
    "total_cleaned_images = sum(validation_results['file_counts'].values())\n",
    "print(f\"Total images in cleaned dataset: {total_cleaned_images:,}\")\n",
    "\n",
    "# Calculate total reduction\n",
    "if 'cleaning_summary' in cleaning_report:\n",
    "    original_total = cleaning_report['cleaning_summary']['total_original_images']\n",
    "    reduction = original_total - total_cleaned_images\n",
    "    reduction_pct = (reduction / original_total) * 100 if original_total > 0 else 0\n",
    "    print(f\"Total reduction: {reduction:,} images ({reduction_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6acd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc25k-cleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
