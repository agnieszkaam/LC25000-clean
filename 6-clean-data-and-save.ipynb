{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44e19b7",
   "metadata": {},
   "source": [
    "# Data Cleaning Pipeline\n",
    "\n",
    "This notebook cleans the LC25000 dataset using the final clustering results from manual annotation. The cleaning process removes duplicates and contaminated samples based on the clustering analysis performed in the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee49b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import project constants\n",
    "from source.constants import (\n",
    "    PROJECT_PATH, DATA_DIR, ALL_CANCER_TYPES, \n",
    "    ANNOTATIONS_SAVE_DIR, NUM_CLASS_PROTOTYPES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EXTRACTOR_NAME = \"UNI\"  # Feature extractor used for clustering\n",
    "IMG_NORM = \"resize_only\"  # Image normalization method used\n",
    "OUTPUT_DIR = os.path.join(PROJECT_PATH, \"LC25000_Clean\")  # Output directory for cleaned dataset\n",
    "\n",
    "print(f\"Project path: {PROJECT_PATH}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Annotations directory: {ANNOTATIONS_SAVE_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Cancer types to process: {ALL_CANCER_TYPES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb660246",
   "metadata": {},
   "source": [
    "## Step 1: Load Clustering Results\n",
    "\n",
    "Load the final clustering results for each cancer type. These clusters were created through the semi-automatic annotation process in notebook 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_final_clusters_csv(cancer_type, extractor_name, img_norm):\n",
    "    \"\"\"Load final clustering results from CSV file for a specific cancer type.\"\"\"\n",
    "    annotations_dir = os.path.join(ANNOTATIONS_SAVE_DIR, cancer_type, extractor_name, img_norm)\n",
    "    final_clusters_csv_path = os.path.join(annotations_dir, \"final_clusters.csv\")\n",
    "    \n",
    "    if not os.path.exists(final_clusters_csv_path):\n",
    "        print(f\"Warning: No clustering results found at {final_clusters_csv_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(final_clusters_csv_path)\n",
    "    \n",
    "    # Group by cluster_label to create clusters dictionary\n",
    "    clusters = {}\n",
    "    for cluster_id, group in df.groupby('cluster_label'):\n",
    "        # Convert relative paths to absolute paths\n",
    "        img_paths = []\n",
    "        for img_path in group['img_path']:\n",
    "            # Remove the './' prefix and make absolute path\n",
    "            clean_path = img_path.replace('./', '')\n",
    "            abs_path = os.path.join(PROJECT_PATH, clean_path)\n",
    "            img_paths.append(abs_path)\n",
    "        clusters[cluster_id] = img_paths\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "# Load clustering results for all cancer types from CSV files\n",
    "all_clusters = {}\n",
    "total_clusters = 0\n",
    "total_images = 0\n",
    "\n",
    "for cancer_type in ALL_CANCER_TYPES:\n",
    "    clusters = load_final_clusters_csv(cancer_type, EXTRACTOR_NAME, IMG_NORM)\n",
    "    if clusters is not None:\n",
    "        all_clusters[cancer_type] = clusters\n",
    "        total_clusters += len(clusters)\n",
    "        total_images += sum(len(cluster_images) for cluster_images in clusters.values())\n",
    "        print(f\"{cancer_type}: {len(clusters)} clusters, {sum(len(cluster_images) for cluster_images in clusters.values())} images\")\n",
    "    else:\n",
    "        print(f\"No clusters found for {cancer_type}\")\n",
    "\n",
    "print(f\"\\nTotal: {total_clusters} clusters, {total_images} images across all cancer types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c0f7c",
   "metadata": {},
   "source": [
    "## Step 2: Analyze Cluster Sizes\n",
    "\n",
    "Analyze the distribution of cluster sizes to understand the data better before cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cluster_sizes(all_clusters):\n",
    "    \"\"\"Analyze the distribution of cluster sizes across all cancer types.\"\"\"\n",
    "    \n",
    "    cluster_stats = {}\n",
    "    \n",
    "    for cancer_type, clusters in all_clusters.items():\n",
    "        cluster_sizes = [len(cluster_images) for cluster_images in clusters.values()]\n",
    "        \n",
    "        stats = {\n",
    "            'total_clusters': len(clusters),\n",
    "            'total_images': sum(cluster_sizes),\n",
    "            'min_size': min(cluster_sizes) if cluster_sizes else 0,\n",
    "            'max_size': max(cluster_sizes) if cluster_sizes else 0,\n",
    "            'mean_size': np.mean(cluster_sizes) if cluster_sizes else 0,\n",
    "            'median_size': np.median(cluster_sizes) if cluster_sizes else 0,\n",
    "            'size_distribution': Counter(cluster_sizes)\n",
    "        }\n",
    "        \n",
    "        cluster_stats[cancer_type] = stats\n",
    "        \n",
    "        print(f\"\\n{cancer_type.upper()}:\")\n",
    "        print(f\"  Total clusters: {stats['total_clusters']}\")\n",
    "        print(f\"  Total images: {stats['total_images']}\")\n",
    "        print(f\"  Cluster size - Min: {stats['min_size']}, Max: {stats['max_size']}\")\n",
    "        print(f\"  Cluster size - Mean: {stats['mean_size']:.2f}, Median: {stats['median_size']:.2f}\")\n",
    "        \n",
    "        # Show size distribution\n",
    "        size_counts = sorted(stats['size_distribution'].items())\n",
    "        print(f\"  Size distribution: {dict(size_counts[:10])}...\")  # Show first 10\n",
    "    \n",
    "    return cluster_stats\n",
    "\n",
    "# Analyze cluster sizes\n",
    "cluster_stats = analyze_cluster_sizes(all_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c46f5",
   "metadata": {},
   "source": [
    "## Step 3: Define Cleaning Strategy\n",
    "\n",
    "Define the strategy for cleaning the dataset:\n",
    "1. **Keep one representative per cluster**: From each cluster, keep only one image (the first one in the list, which is typically the cluster centroid)\n",
    "2. **Remove duplicates**: This eliminates duplicate and near-duplicate images\n",
    "3. **Maintain class balance**: Ensure we keep a balanced number of samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_representative_images(all_clusters, selection_strategy=\"first\"):\n",
    "    \"\"\"\n",
    "    Select representative images from each cluster based on the final_clusters.csv data.\n",
    "    \n",
    "    Args:\n",
    "        all_clusters: Dictionary of clusters for each cancer type (from CSV)\n",
    "        selection_strategy: Strategy for selecting representative (\"first\", \"random\", \"median\")\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with selected representative images for each cancer type\n",
    "    \"\"\"\n",
    "    \n",
    "    representatives = {}\n",
    "    \n",
    "    for cancer_type, clusters in all_clusters.items():\n",
    "        selected_images = []\n",
    "        \n",
    "        for cluster_id, cluster_images in clusters.items():\n",
    "            if len(cluster_images) > 0:\n",
    "                if selection_strategy == \"first\":\n",
    "                    # Select the first image in the cluster\n",
    "                    representative = cluster_images[0]\n",
    "                elif selection_strategy == \"random\":\n",
    "                    # Select a random image from the cluster\n",
    "                    representative = np.random.choice(cluster_images)\n",
    "                elif selection_strategy == \"median\":\n",
    "                    # Select the median image (middle index)\n",
    "                    median_idx = len(cluster_images) // 2\n",
    "                    representative = cluster_images[median_idx]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown selection strategy: {selection_strategy}\")\n",
    "                \n",
    "                selected_images.append((representative, cluster_id))  # Include cluster_id for naming\n",
    "        \n",
    "        representatives[cancer_type] = selected_images\n",
    "        print(f\"{cancer_type}: Selected {len(selected_images)} representative images from {len(clusters)} clusters\")\n",
    "    \n",
    "    return representatives\n",
    "\n",
    "# Select representative images (using first strategy)\n",
    "representative_images = select_representative_images(all_clusters, selection_strategy=\"first\")\n",
    "\n",
    "# Print summary\n",
    "total_representatives = sum(len(images) for images in representative_images.values())\n",
    "print(f\"\\nTotal representative images selected: {total_representatives}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0e220d",
   "metadata": {},
   "source": [
    "## Step 4: Create Cleaned Dataset\n",
    "\n",
    "Copy the selected representative images to create the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cleaned_dataset(representative_images, output_dir, include_cluster_id=True):\n",
    "    \"\"\"\n",
    "    Create a cleaned dataset by copying representative images from clusters.\n",
    "    \n",
    "    Args:\n",
    "        representative_images: Dictionary of (image_path, cluster_id) tuples for each cancer type\n",
    "        output_dir: Output directory for cleaned dataset\n",
    "        include_cluster_id: Whether to include cluster ID in the filename\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory structure\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    copy_stats = {}\n",
    "    \n",
    "    for cancer_type, image_data in representative_images.items():\n",
    "        # Create cancer type directory\n",
    "        cancer_type_dir = os.path.join(output_dir, cancer_type)\n",
    "        os.makedirs(cancer_type_dir, exist_ok=True)\n",
    "        \n",
    "        copied_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        print(f\"\\nProcessing {cancer_type}...\")\n",
    "        \n",
    "        for image_path, cluster_id in tqdm(image_data, desc=f\"Copying {cancer_type}\"):\n",
    "            try:\n",
    "                # Extract filename and extension\n",
    "                filename = os.path.basename(image_path)\n",
    "                name, ext = os.path.splitext(filename)\n",
    "                \n",
    "                # Create new filename with cluster ID if requested\n",
    "                if include_cluster_id:\n",
    "                    new_filename = f\"cluster_{cluster_id:03d}_{name}{ext}\"\n",
    "                else:\n",
    "                    new_filename = filename\n",
    "                \n",
    "                # Source path\n",
    "                source_path = image_path\n",
    "                \n",
    "                # Destination path\n",
    "                dest_path = os.path.join(cancer_type_dir, new_filename)\n",
    "                \n",
    "                # Copy the file\n",
    "                if os.path.exists(source_path):\n",
    "                    shutil.copy2(source_path, dest_path)\n",
    "                    copied_count += 1\n",
    "                else:\n",
    "                    print(f\"Warning: Source file not found: {source_path}\")\n",
    "                    failed_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error copying {image_path}: {e}\")\n",
    "                failed_count += 1\n",
    "        \n",
    "        copy_stats[cancer_type] = {\n",
    "            'copied': copied_count,\n",
    "            'failed': failed_count,\n",
    "            'total': len(image_data)\n",
    "        }\n",
    "        \n",
    "        print(f\"{cancer_type}: Copied {copied_count}/{len(image_data)} images\")\n",
    "    \n",
    "    return copy_stats\n",
    "\n",
    "# Create the cleaned dataset with cluster IDs in filenames\n",
    "print(f\"Creating cleaned dataset in: {OUTPUT_DIR}\")\n",
    "copy_statistics = create_cleaned_dataset(representative_images, OUTPUT_DIR, include_cluster_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad6b5b",
   "metadata": {},
   "source": [
    "## Step 5: Generate Cleaning Report\n",
    "\n",
    "Create a comprehensive report of the cleaning process including statistics and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaede92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cleaning_report(cluster_stats, copy_statistics, output_dir):\n",
    "    \"\"\"Generate a comprehensive cleaning report.\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'cleaning_metadata': {\n",
    "            'extractor_used': EXTRACTOR_NAME,\n",
    "            'normalization_used': IMG_NORM,\n",
    "            'selection_strategy': 'first',  # cluster centroid\n",
    "            'date_cleaned': pd.Timestamp.now().isoformat(),\n",
    "            'source_directory': DATA_DIR,\n",
    "            'output_directory': output_dir\n",
    "        },\n",
    "        'original_dataset_stats': {},\n",
    "        'clustering_stats': cluster_stats,\n",
    "        'cleaned_dataset_stats': copy_statistics,\n",
    "        'cleaning_summary': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    total_original = sum(stats['total_images'] for stats in cluster_stats.values())\n",
    "    total_cleaned = sum(stats['copied'] for stats in copy_statistics.values())\n",
    "    reduction_ratio = 1 - (total_cleaned / total_original) if total_original > 0 else 0\n",
    "    \n",
    "    report['cleaning_summary'] = {\n",
    "        'total_original_images': total_original,\n",
    "        'total_cleaned_images': total_cleaned,\n",
    "        'images_removed': total_original - total_cleaned,\n",
    "        'reduction_ratio': reduction_ratio,\n",
    "        'compression_ratio': total_cleaned / total_original if total_original > 0 else 0\n",
    "    }\n",
    "    \n",
    "    # Save report as JSON\n",
    "    report_path = os.path.join(output_dir, 'cleaning_report.json')\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"CLEANING REPORT SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Original dataset: {total_original:,} images\")\n",
    "    print(f\"Cleaned dataset: {total_cleaned:,} images\")\n",
    "    print(f\"Images removed: {total_original - total_cleaned:,} ({reduction_ratio:.2%})\")\n",
    "    print(f\"Compression ratio: {total_cleaned / total_original:.3f}\" if total_original > 0 else \"N/A\")\n",
    "    \n",
    "    print(\"\\\\nPer-class statistics:\")\n",
    "    for cancer_type in ALL_CANCER_TYPES:\n",
    "        if cancer_type in cluster_stats and cancer_type in copy_statistics:\n",
    "            original = cluster_stats[cancer_type]['total_images']\n",
    "            cleaned = copy_statistics[cancer_type]['copied']\n",
    "            print(f\"  {cancer_type}: {original:,} â†’ {cleaned:,} ({cleaned/original:.3f})\")\n",
    "    \n",
    "    print(f\"\\\\nDetailed report saved to: {report_path}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate cleaning report\n",
    "cleaning_report = generate_cleaning_report(cluster_stats, copy_statistics, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17e1a45",
   "metadata": {},
   "source": [
    "## Step 6: Create Image Mapping\n",
    "\n",
    "Create a mapping file that tracks which original images were kept in the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_mapping(representative_images, all_clusters, output_dir):\n",
    "    \"\"\"\n",
    "    Create a detailed mapping of which images were kept and which clusters they came from.\n",
    "    \"\"\"\n",
    "    \n",
    "    mapping_data = []\n",
    "    \n",
    "    for cancer_type, image_data in representative_images.items():\n",
    "        clusters = all_clusters[cancer_type]\n",
    "        \n",
    "        # Create mapping for representatives\n",
    "        for image_path, cluster_id in image_data:\n",
    "            cluster_images = clusters[cluster_id]\n",
    "            \n",
    "            # Create mapping entry\n",
    "            mapping_entry = {\n",
    "                'cancer_type': cancer_type,\n",
    "                'cluster_id': cluster_id,\n",
    "                'representative_image': image_path,\n",
    "                'cluster_size': len(cluster_images),\n",
    "                'duplicate_images': [img for img in cluster_images if img != image_path],\n",
    "                'kept_in_cleaned_dataset': True\n",
    "            }\n",
    "            \n",
    "            mapping_data.append(mapping_entry)\n",
    "    \n",
    "    # Create DataFrame for easy analysis\n",
    "    mapping_df = pd.DataFrame(mapping_data)\n",
    "    \n",
    "    # Save as CSV\n",
    "    mapping_csv_path = os.path.join(output_dir, 'image_mapping.csv')\n",
    "    mapping_df.to_csv(mapping_csv_path, index=False)\n",
    "    \n",
    "    # Save detailed mapping as JSON\n",
    "    mapping_json_path = os.path.join(output_dir, 'image_mapping.json')\n",
    "    with open(mapping_json_path, 'w') as f:\n",
    "        json.dump(mapping_data, f, indent=4)\n",
    "    \n",
    "    print(f\"Image mapping saved to:\")\n",
    "    print(f\"  CSV: {mapping_csv_path}\")\n",
    "    print(f\"  JSON: {mapping_json_path}\")\n",
    "    \n",
    "    # Print some statistics\n",
    "    total_clusters = len(mapping_df)\n",
    "    avg_cluster_size = mapping_df['cluster_size'].mean()\n",
    "    max_cluster_size = mapping_df['cluster_size'].max()\n",
    "    min_cluster_size = mapping_df['cluster_size'].min()\n",
    "    \n",
    "    print(f\"\\nMapping Statistics:\")\n",
    "    print(f\"  Total clusters: {total_clusters}\")\n",
    "    print(f\"  Average cluster size: {avg_cluster_size:.2f}\")\n",
    "    print(f\"  Cluster size range: {min_cluster_size} - {max_cluster_size}\")\n",
    "    \n",
    "    return mapping_df\n",
    "\n",
    "# Create image mapping\n",
    "mapping_df = create_image_mapping(representative_images, all_clusters, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61776d38",
   "metadata": {},
   "source": [
    "## Step 7: Validation and Quality Checks\n",
    "\n",
    "Perform validation checks on the cleaned dataset to ensure everything was processed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_cleaned_dataset(output_dir, expected_stats):\n",
    "    \"\"\"\n",
    "    Validate the cleaned dataset by checking file counts and integrity.\n",
    "    \"\"\"\n",
    "    \n",
    "    validation_results = {\n",
    "        'validation_passed': True,\n",
    "        'issues': [],\n",
    "        'file_counts': {},\n",
    "        'missing_files': [],\n",
    "        'unexpected_files': []\n",
    "    }\n",
    "    \n",
    "    print(\"Validating cleaned dataset...\")\n",
    "    \n",
    "    # Check if output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        validation_results['validation_passed'] = False\n",
    "        validation_results['issues'].append(f\"Output directory does not exist: {output_dir}\")\n",
    "        return validation_results\n",
    "    \n",
    "    # Check each cancer type directory\n",
    "    for cancer_type in ALL_CANCER_TYPES:\n",
    "        cancer_dir = os.path.join(output_dir, cancer_type)\n",
    "        \n",
    "        if not os.path.exists(cancer_dir):\n",
    "            # This is not a failure if no images were expected for this class\n",
    "            if cancer_type not in expected_stats or expected_stats[cancer_type]['copied'] == 0:\n",
    "                continue\n",
    "            validation_results['validation_passed'] = False\n",
    "            validation_results['issues'].append(f\"Cancer type directory missing: {cancer_dir}\")\n",
    "            continue\n",
    "        \n",
    "        # Count files in the directory\n",
    "        files = [f for f in os.listdir(cancer_dir) if os.path.isfile(os.path.join(cancer_dir, f))]\n",
    "        file_count = len(files)\n",
    "        validation_results['file_counts'][cancer_type] = file_count\n",
    "        \n",
    "        # Check against expected count\n",
    "        if cancer_type in expected_stats:\n",
    "            expected_count = expected_stats[cancer_type]['copied']\n",
    "            if file_count != expected_count:\n",
    "                validation_results['validation_passed'] = False\n",
    "                validation_results['issues'].append(\n",
    "                    f\"{cancer_type}: Expected {expected_count} files, found {file_count}\"\n",
    "                )\n",
    "        \n",
    "        print(f\"  {cancer_type}: {file_count} files\")\n",
    "    \n",
    "    # Check for required metadata files\n",
    "    required_files = ['cleaning_report.json', 'image_mapping.csv', 'image_mapping.json']\n",
    "    for req_file in required_files:\n",
    "        file_path = os.path.join(output_dir, req_file)\n",
    "        if not os.path.exists(file_path):\n",
    "            validation_results['validation_passed'] = False\n",
    "            validation_results['issues'].append(f\"Required file missing: {req_file}\")\n",
    "    \n",
    "    # Summary\n",
    "    if validation_results['validation_passed']:\n",
    "        print(\"\\nValidation PASSED - Dataset cleaned successfully!\")\n",
    "    else:\n",
    "        print(\"\\nValidation FAILED - Issues found:\")\n",
    "        for issue in validation_results['issues']:\n",
    "            print(f\"   - {issue}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Validate the cleaned dataset\n",
    "validation_results = validate_cleaned_dataset(OUTPUT_DIR, copy_statistics)\n",
    "\n",
    "# Print final summary\n",
    "print(f\"\\nCleaned dataset location: {OUTPUT_DIR}\")\n",
    "total_cleaned_images = sum(validation_results['file_counts'].values())\n",
    "print(f\"Total images in cleaned dataset: {total_cleaned_images:,}\")\n",
    "\n",
    "# Calculate total reduction\n",
    "if 'cleaning_summary' in cleaning_report:\n",
    "    original_total = cleaning_report['cleaning_summary']['total_original_images']\n",
    "    reduction = original_total - total_cleaned_images\n",
    "    reduction_pct = (reduction / original_total) * 100 if original_total > 0 else 0\n",
    "    print(f\"Total reduction: {reduction:,} images ({reduction_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6acd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc25k-cleaning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
